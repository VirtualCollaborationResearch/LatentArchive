{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2 as cv\n",
    "from dataset import CustomImageDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage import exposure\n",
    "from skimage.exposure import match_histograms\n",
    "import ImageProcessing as IP\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring image directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_dir ='./Sample RGB/'\n",
    "Depth_dir = './Sample Depth/'\n",
    "ref_dir = './Manhatta Frames/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(img_dir, depth_dir):\n",
    "    \"\"\"Function that reads images from a given directory\"\"\"\n",
    "    \n",
    "    img_path = Path(img_dir)\n",
    "    depth_path = Path(depth_dir)\n",
    "    \n",
    "    img_list = []\n",
    "    depth_list = []\n",
    "\n",
    "    for img in img_path.iterdir():\n",
    "\n",
    "        image = cv.imread(img_dir + '/' + img.name, cv.IMREAD_ANYDEPTH + cv.IMREAD_COLOR)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        image = IP.rgb_transform(image)\n",
    "        image = IP.histogram_matching(image, ref_dir)\n",
    "        img_list.append(image)\n",
    "\n",
    "    for depth in depth_path.iterdir():\n",
    "\n",
    "        depth = cv.imread(depth_dir + '/' + depth.name, cv.IMREAD_ANYDEPTH + cv.IMREAD_GRAYSCALE)\n",
    "        depth_list.append(depth)\n",
    "\n",
    "    return img_list, depth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data, labels = read_images(RGB_dir, Depth_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#train_ims, test_ims, train_labels, test_labels = train_test_split(data, labels, test_size=.2, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all train and validation images into designated paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,train_img in enumerate(train_ims):\n",
    "#    cv.imwrite('./Monocular-Depth-Estimation-Toolbox/data/custom_dataset/train/rgb/' + str(i) + '.jpeg' , np.array(train_img).transpose(1,2,0))\n",
    "#    cv.imwrite('./Monocular-Depth-Estimation-Toolbox/data/custom_dataset/train/depth/' + str(i) + '.jpeg' , np.array(train_labels[i]))\n",
    "#\n",
    "#for i,test_img in enumerate(test_ims):\n",
    "#    cv.imwrite('./Monocular-Depth-Estimation-Toolbox/data/custom_dataset/val/rgb/' + str(i) + '.jpeg' , np.array(test_img).transpose(1,2,0))\n",
    "#    cv.imwrite('./Monocular-Depth-Estimation-Toolbox/data/custom_dataset/val/depth/' + str(i) + '.jpeg' , np.array(test_labels[i]))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(IP)\n",
    "#\n",
    "#test_img = IP.rgb_transform(rgb_img[0])\n",
    "#plt.imshow(test_img.transpose(0,2).transpose(0,1), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matched_img = IP.histogram_matching(test_img, ref_dir)\n",
    "#print(np.array(matched_img.shape[::-1]))\n",
    "#matched_img.transpose_(0,2).transpose_(0,1)\n",
    "#plt.imshow(matched_img.numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing a scene for consistent depth prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning huggingface model: Intel/dpt-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoFeatureExtractor, DPTForDepthEstimation\n",
    "#from PIL import Image\n",
    "#\n",
    "#\n",
    "#model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CustomImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = CustomImageDataset(train_ims, train_labels, transform=IP.rgb_transform, histogram_matching=IP.histogram_matching)\n",
    "#test_dataset = CustomImageDataset(test_ims, test_labels, transform=IP.rgb_transform, histogram_matching=IP.histogram_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import TrainingArguments, Trainer\n",
    "#from datasets import load_metric\n",
    "#\n",
    "#training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "#metric = load_metric(\"accuracy\")\n",
    "#\n",
    "#def compute_metrics(eval_pred):\n",
    "#    logits, labels = eval_pred\n",
    "#    predictions = np.argmax(logits, axis=-1)\n",
    "#    return metric.compute(predictions=predictions, references=labels)\n",
    "#\n",
    "#training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "#\n",
    "#inputs = np.empty(rgb_img.shape)\n",
    "#for i in range(0,rgb_img.shape[0]):\n",
    "#    inputs[i] = extractor(images=IP.transform_to_PIL(rgb_img[0]), return_tensors=\"pt\")\n",
    "#\n",
    "#trainer = Trainer(\n",
    "#    model=model,\n",
    "#    args=training_args,\n",
    "#    train_dataset= extractor(images=IP.transform_to_PIL(depth_img[0]), return_tensors=\"pt\"),\n",
    "#    eval_dataset=depth_img[0],\n",
    "#    #compute_metrics=compute_metrics,\n",
    "#)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import TrainingArguments, Trainer\n",
    "#inputs = extractor(images=rgb_img[0], return_tensors=\"pt\")\n",
    "#\n",
    "#with torch.no_grad():\n",
    "#    outputs = model(**inputs)\n",
    "#    predicted_depth = outputs.predicted_depth\n",
    "#print(type(predicted_depth))\n",
    "#\n",
    "#prediction = torch.nn.functional.interpolate(\n",
    "#    predicted_depth.unsqueeze(0),\n",
    "#    size=rgb_img.size,\n",
    "#    mode=\"bicubic\",\n",
    "#    align_corners=False,\n",
    "#)\n",
    "#\n",
    "## visualize the prediction\n",
    "#output = prediction.squeeze().cpu().numpy()\n",
    "#formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "#depth = Image.fromarray(formatted)\n",
    "#\n",
    "#depth.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depth_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ardaturkyasar/Documents/repos/LatentArchive/main.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ardaturkyasar/Documents/repos/LatentArchive/main.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_dataset[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset[0]['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset= test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 168\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f493e3a3464adab3e5f84bfd230b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('LatentArchive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be5bfbfb43e052378433cac072f4ed5858ca06f281fafca0cb7e4357145b37e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
