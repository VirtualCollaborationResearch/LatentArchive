{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2 as cv\n",
    "from dataset import CustomImageDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage import exposure\n",
    "from skimage.exposure import match_histograms\n",
    "import ImageProcessing as IP\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring image directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_dir ='./Sample RGB/'\n",
    "Depth_dir = './Sample Depth/'\n",
    "ref_dir = './Manhatta Frames/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1080, 1920])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(IP)\n",
    "\n",
    "data = CustomImageDataset(img_dir= RGB_dir, depth_dir= Depth_dir, transform= IP.rgb_transform, histogram_matching= IP.histogram_matching)\n",
    "data_loader = DataLoader(data, batch_size=2, shuffle=True) #Loads a random RGB, Depth couple from the dataset\n",
    "rgb_img, depth_img = next(iter(data_loader)) #Unsqueezes and converts to tensor, may need to squeeze(0)\n",
    "plt.imshow(rgb_img[0][0,:,:].squeeze().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(depth_img[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1080, 1920])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(IP)\n",
    "#\n",
    "#test_img = IP.rgb_transform(rgb_img[0])\n",
    "#plt.imshow(test_img.transpose(0,2).transpose(0,1), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matched_img = IP.histogram_matching(test_img, ref_dir)\n",
    "#print(np.array(matched_img.shape[::-1]))\n",
    "#matched_img.transpose_(0,2).transpose_(0,1)\n",
    "#plt.imshow(matched_img.numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing a scene for consistent depth prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,img in enumerate(rgb_img):\n",
    "    cv.imwrite('./consistent_depth-main/color_full/' + str(i) + '.jpeg' , rgb_img[i].squeeze().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning huggingface model: Intel/dpt-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, DPTForDepthEstimation\n",
    "from PIL import Image\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'pixel_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Latent Archive\\LatentArchive\\main.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m inputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(rgb_img\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,rgb_img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     inputs[i] \u001b[39m=\u001b[39m extractor(images\u001b[39m=\u001b[39mIP\u001b[39m.\u001b[39mtransform_to_PIL(rgb_img[\u001b[39m0\u001b[39m]), return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'pixel_values'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "inputs = np.empty(rgb_img.shape)\n",
    "for i in range(0,rgb_img.shape[0]):\n",
    "    inputs[i] = extractor(images=IP.transform_to_PIL(rgb_img[0]), return_tensors=\"pt\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= extractor(images=IP.transform_to_PIL(depth_img[0]), return_tensors=\"pt\"),\n",
    "    eval_dataset=depth_img[0],\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f825b4ed747b4104aa3253599a17e35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'Indexing with integers is not available when using Python based feature extractors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Latent Archive\\LatentArchive\\main.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1503\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1714\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1713\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1714\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1715\u001b[0m \n\u001b[0;32m   1716\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1717\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1718\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 517\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    518\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    520\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    521\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    556\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    559\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\transformers\\feature_extraction_utils.py:89\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Indexing with integers is not available when using Python based feature extractors'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "upsample_bicubic2d(): argument 'output_size' must be tuple of ints, but found element of type builtin_function_or_method at pos 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Latent Archive\\LatentArchive\\main.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     predicted_depth \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mpredicted_depth\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(predicted_depth))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m prediction \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49minterpolate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     predicted_depth\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     size\u001b[39m=\u001b[39;49mrgb_img\u001b[39m.\u001b[39;49msize,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbicubic\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# visualize the prediction\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m output \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3560\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[0;32m   3558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   3559\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 3560\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_bicubic2d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[0;32m   3562\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   3563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGot 3D input, but bilinear mode needs 4D input\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: upsample_bicubic2d(): argument 'output_size' must be tuple of ints, but found element of type builtin_function_or_method at pos 1"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "inputs = extractor(images=rgb_img[0], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "print(type(predicted_depth))\n",
    "\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    predicted_depth.unsqueeze(0),\n",
    "    size=rgb_img.size,\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "# visualize the prediction\n",
    "output = prediction.squeeze().cpu().numpy()\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = Image.fromarray(formatted)\n",
    "\n",
    "depth.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1080, 1920])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't assign a torch.Size to a torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Latent Archive\\LatentArchive\\main.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Latent%20Archive/LatentArchive/main.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m depth_img[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((depth_img[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), depth_img[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), depth_img[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)))\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mTypeError\u001b[0m: can't assign a torch.Size to a torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "torch.cat((depth_img[0].unsqueeze(0), depth_img[0].unsqueeze(0), depth_img[0].unsqueeze(0))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "torch.Size([3, 480, 640]) torch.Size([3, 1080, 1920])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-large\")\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "print(type(image))\n",
    "# prepare image for the model\n",
    "\n",
    "\n",
    "convert = transforms.PILToTensor()\n",
    "print(convert(image).shape, rgb_img[0].shape)\n",
    "inputs = feature_extractor(images=rgb_img[0], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "# interpolate to original size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    predicted_depth.unsqueeze(1),\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "# visualize the prediction\n",
    "output = prediction.squeeze().cpu().numpy()\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = Image.fromarray(formatted)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= extractor(images=rgb_img[0], return_tensors=\"pt\"),\n",
    "    eval_dataset=torch.cat((depth_img[0].unsqueeze(0), depth_img[0].unsqueeze(0), depth_img[0].unsqueeze(0))).shape,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
